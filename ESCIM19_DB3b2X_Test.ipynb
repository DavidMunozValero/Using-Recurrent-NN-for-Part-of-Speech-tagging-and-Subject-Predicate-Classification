{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESCIM19_DB3b2X_Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TraktylCo/Using-Recurrent-NN-for-Part-of-Speech-tagging-and-Subject-Predicate-Classification/blob/master/ESCIM19_DB3b2X_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwOlnaZapcVY",
        "colab_type": "code",
        "outputId": "1356ee03-eadb-4189-9e5c-723c2d49d267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount GDrive Data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOH3H0NJ-rEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "intervals = (\n",
        "    ('weeks', 604800),  # 60 * 60 * 24 * 7\n",
        "    ('days', 86400),    # 60 * 60 * 24\n",
        "    ('hours', 3600),    # 60 * 60\n",
        "    ('minutes', 60),\n",
        "    ('seconds', 1),\n",
        "    )\n",
        "\n",
        "def display_time(seconds, granularity=2):\n",
        "    result = []\n",
        "\n",
        "    for name, count in intervals:\n",
        "        value = seconds // count\n",
        "        if value:\n",
        "            seconds -= value * count\n",
        "            if value == 1:\n",
        "                name = name.rstrip('s')\n",
        "            result.append(\"{} {}\".format(value, name))\n",
        "    return ', '.join(result[:granularity])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtgZPQhpgXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ************************************************************\n",
        "# *******************  TEST MODEL  ***************************\n",
        "# ************************************************************\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "\n",
        "# Load models\n",
        "model1 = load_model('/content/gdrive/My Drive/datasets/PM_ESCIM19_DB3b_N1.h5')\n",
        "model1.compile(loss='sparse_categorical_crossentropy', optimizer=RMSprop(lr=0.0001), metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "model2 = load_model('/content/gdrive/My Drive/datasets/PM_ESCIM19_DB3b_N2.h5')\n",
        "model2.compile(loss='sparse_categorical_crossentropy', optimizer=RMSprop(lr=0.0001), metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "# Load dictionaries\n",
        "word_index = np.load('/content/gdrive/My Drive/datasets/word_index_db3b.npy', allow_pickle=True).item()\n",
        "pos_index = np.load('/content/gdrive/My Drive/datasets/pos_index_db3b.npy', allow_pickle=True).item()\n",
        "sp_index = np.load('/content/gdrive/My Drive/datasets/sp_index_db3b.npy', allow_pickle=True).item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzc48u5gpxNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load test data sets\n",
        "\n",
        "test_sentences = np.load('/content/gdrive/My Drive/datasets/test_sentences_db3b.npy', allow_pickle=True).tolist()\n",
        "test_pos = np.load('/content/gdrive/My Drive/datasets/test_pos_db3b.npy', allow_pickle=True).tolist()\n",
        "test_sp = np.load('/content/gdrive/My Drive/datasets/test_sp_db3b.npy', allow_pickle=True).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMwSofX_qljL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build test data\n",
        "\n",
        "test_enc_sent = []\n",
        "test_enc_pos = []\n",
        "\n",
        "for s in test_sentences:\n",
        "    test_enc_sent.append([word_index[word.lower()] for word in s])     \n",
        "\n",
        "for s in test_pos:\n",
        "    test_enc_pos.append([pos_index[word.lower()] for word in s])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfm0T9V_QhWh",
        "colab_type": "code",
        "outputId": "30f79dae-9f70-4adb-9f65-ecf52edaaba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Convert data to numpy array\n",
        "test_enc_sent = np.array(test_enc_sent)\n",
        "test_enc_pos = np.array(test_enc_pos)\n",
        "\n",
        "for i, v in enumerate(test_enc_sent):\n",
        "    v = np.array(v)\n",
        "    test_enc_sent[i] = v\n",
        "    \n",
        "for i, v in enumerate(test_enc_pos):\n",
        "    v = np.array(v)\n",
        "    test_enc_pos[i] = v\n",
        "\n",
        "# Show data shape (it only shows first dimension, since the others are different for each sequence)\n",
        "print(test_enc_sent.shape)\n",
        "print(test_enc_pos.shape)\n",
        "\n",
        "for i in range(len(test_enc_sent)):\n",
        "  test_enc_sent[i] = test_enc_sent[i].reshape(1, len(test_enc_sent[i]), 1)\n",
        "  test_enc_pos[i] = test_enc_pos[i].reshape(1, len(test_enc_pos[i]), 1)\n",
        "\n",
        "for i in range(len(test_enc_sent)):\n",
        "  test_enc_sent[i] = np.array(test_enc_sent[i])\n",
        "  test_enc_pos[i] = np.array(test_enc_pos[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2633,)\n",
            "(2633,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWaKWBH9rGTx",
        "colab_type": "code",
        "outputId": "b01c5a7a-8659-4788-c1a0-99ab2198d020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# RESULTS NETWORK 1\n",
        "\n",
        "# Variable to store accuracy of Network\n",
        "network_accs = []\n",
        "\n",
        "# Function to decode network prediction\n",
        "def logits_to_tokens(sequences, index):\n",
        "  token_sequences = []\n",
        "  for categorical_sequence in sequences:\n",
        "    token_sequence = []\n",
        "    for categorical in categorical_sequence:\n",
        "      token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "      token_sequences.append(token_sequence)\n",
        "    \n",
        "  return token_sequences\n",
        "\n",
        "print(\"Size of test set: \", len(test_sentences))\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "model1.predict(test_enc_sent[0], 1)\n",
        "\n",
        "for i in range(len(test_sentences)):\n",
        "  \n",
        "  # Network prediction requires encoded data\n",
        "  prediction = model1.predict(test_enc_sent[i], 1)\n",
        "\n",
        "  # Decode the network prediction\n",
        "  network_results = logits_to_tokens(prediction, {i: t for t, i in pos_index.items()})[0]\n",
        "\n",
        "  count_network = 0\n",
        "\n",
        "  # Check number of correctly identified tags from Network results\n",
        "  for j in range(len(test_pos[i])):\n",
        "    if test_pos[i][j].lower() == network_results[j]:\n",
        "      count_network += 1\n",
        "\n",
        "  acc_log_network = count_network / len(test_sentences[i])\n",
        "  network_accs.append(acc_log_network)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "elapsed_time = end - start\n",
        "\n",
        "# Calculate final accuracy (average)\n",
        "network_acc = sum(network_accs) / len(test_sentences)\n",
        "\n",
        "print(\"Network Accuracy: \", network_acc*100, \"%\")\n",
        "print(\"Elapsed time: \", elapsed_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of test set:  2633\n",
            "Network Accuracy:  90.38760796242971 %\n",
            "Elapsed time:  8.940792798995972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL3y452wChd2",
        "colab_type": "code",
        "outputId": "cfd82d46-1db8-4415-f430-bcfc46eb1fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# TEST TIME NETWORK\n",
        "\n",
        "print(\"Size of test set: \", len(test_sentences))\n",
        "\n",
        "model1.predict(test_enc_sent[0], 1)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "for i in range(len(test_sentences)):\n",
        "  \n",
        "  # Network prediction requires encoded data\n",
        "  prediction = model1.predict(test_enc_sent[i], 1)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "elapsed_time = end - start\n",
        "\n",
        "print(\"Elapsed time: \", elapsed_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of test set:  2633\n",
            "Elapsed time:  8.86074709892273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndboFo_R_JBr",
        "colab_type": "code",
        "outputId": "52d3e949-495b-479d-d661-bd089d573199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# RESULTS NLTK POS TAG\n",
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Variable to store accuracy of NLTK\n",
        "nltk_accs = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(test_sentences)):\n",
        "  # NLTK receives sentences (without encoding)\n",
        "  nltk_results = nltk.pos_tag(test_sentences[i])\n",
        "\n",
        "  # Check number of correctly identified tags from NLTK results\n",
        "  for k in range(len(nltk_results)):\n",
        "    nltk_results[k] = nltk_results[k][1]\n",
        "\n",
        "  count_nltk = 0\n",
        "\n",
        "  # Check number of correctly identified tags from Network results\n",
        "  for j in range(len(test_pos[i])):\n",
        "    if test_pos[i][j] == nltk_results[j]:\n",
        "      count_nltk += 1\n",
        "\n",
        "  # Calculate accuracy for each sentence, and append to accuracy logs\n",
        "  acc_log_nltk = count_nltk / len(test_sentences[i])\n",
        "  nltk_accs.append(acc_log_nltk)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "elapsed_time = end - start\n",
        "\n",
        "# Calculate final accuracy (average)\n",
        "nltk_acc = sum(nltk_accs) / len(test_sentences)\n",
        "\n",
        "print(\"NLTK POS tagger: \", nltk_acc*100, \"%\")\n",
        "print(\"Elapsed time: \", elapsed_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "NLTK POS tagger:  90.15603326518324 %\n",
            "Elapsed time:  1.0786750316619873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iudj1jhNUAkj",
        "colab_type": "code",
        "outputId": "f92da64e-537a-4af6-eaf4-42647d714cd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Difference: \", (network_acc-nltk_acc)*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Difference:  0.23157469724648383 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L0hzLERM5tz",
        "colab_type": "code",
        "outputId": "0292f0fc-382b-4469-cad7-47813a0ccfe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install git+git://github.com/emilmont/pyStatParser"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/emilmont/pyStatParser\n",
            "  Cloning git://github.com/emilmont/pyStatParser to /tmp/pip-req-build-wci8jlde\n",
            "  Running command git clone -q git://github.com/emilmont/pyStatParser /tmp/pip-req-build-wci8jlde\n",
            "Building wheels for collected packages: pyStatParser\n",
            "  Building wheel for pyStatParser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyStatParser: filename=pyStatParser-0.0.1-cp36-none-any.whl size=720546 sha256=eb4e1fb712b396556904f3a3051f7eb0da7e39b8139aaf03638eed8f71142e3d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vh35c9sk/wheels/3f/c7/73/8cdd13678ef0f1d3bcc15d8b5c992662d77c32dab69fea8504\n",
            "Successfully built pyStatParser\n",
            "Installing collected packages: pyStatParser\n",
            "Successfully installed pyStatParser-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l1QRB1ANKOw",
        "colab_type": "code",
        "outputId": "f87d25f0-b46f-4ba4-cf55-1662520c2dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from stat_parser import Parser\n",
        "from nltk import Tree\n",
        "\n",
        "exceptions = []\n",
        "print(len(test_sentences))\n",
        "parser = Parser()\n",
        "\n",
        "sentences = []\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "  print(i)\n",
        "  \n",
        "  try:\n",
        "    tree = parser.parse(str(\" \".join(sentence)))\n",
        "  except:\n",
        "    print(\"Exception!\")\n",
        "    exceptions.append(i)\n",
        "\n",
        "  sentences.append(tree)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2633\n",
            "Building the Grammar Model\n",
            "Time: (2.70)s\n",
            "\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "Exception!\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "Exception!\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "Exception!\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "Exception!\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "Exception!\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "Exception!\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "Exception!\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "Exception!\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "Exception!\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-eX45E8BbtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save('pystat_results.npy', sentences)\n",
        "np.save('exceptions.npy', exceptions)\n",
        "\n",
        "sentences = np.load('pystat_results_db3b.npy', allow_pickle=True).tolist()\n",
        "exceptions = np.load('exceptions_db3b.npy', allow_pickle=True).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnJygTDdRIUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = np.load('pystat_results_db3b.npy', allow_pickle=True).tolist()\n",
        "exceptions = np.load('exceptions_db3b.npy', allow_pickle=True).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkd8ft_-MHc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in reversed(range(len(test_sentences))):\n",
        "  if i in exceptions:\n",
        "    sentences.pop(i)\n",
        "    test_enc_pos.pop(i)\n",
        "    test_sp.pop(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lgY8lA9Yp2P",
        "colab_type": "code",
        "outputId": "5685ae83-dcf7-4121-9ff5-fa285f1d1dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(test_sp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eteTLQAHNbc9",
        "colab_type": "code",
        "outputId": "169a77b2-10c2-48d6-c1b1-15c0ad71da54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read dataset & build data variables\n",
        "import nltk\n",
        "\n",
        "# Global variables\n",
        "results_pystat = [[] for x in range(len(sentences))] # Save sentences tags\n",
        "tag = \"\"\n",
        "buff = \"\"\n",
        "count = 0\n",
        "\n",
        "# Recursive iteration through NLTK Trees\n",
        "def iterate_tree(sent):\n",
        "    \n",
        "    global tag\n",
        "  \n",
        "    if not tag.endswith(\"_\"):\n",
        "        tag += \"_\"\n",
        "    \n",
        "    buff = str(sent.label())\n",
        "    buff = buff.split(\"-\")[0]\n",
        "\n",
        "    tag += buff + \"_\"\n",
        "    buff = \"\"\n",
        "    \n",
        "    for tok in sent:\n",
        "        if type(tok) is nltk.tree.Tree:\n",
        "            iterate_tree(tok)\n",
        "        elif type(tok) is str:\n",
        "            results_pystat[count].append(tag.split('_')[1:-1])\n",
        "        \n",
        "        tag = tag.split('_')\n",
        "        tag = tag[:-1]\n",
        "        tag = '_'.join(tag)\n",
        "\n",
        "for sentence in sentences:\n",
        "    iterate_tree(sentence)\n",
        "    tag = \"\"\n",
        "    count += 1\n",
        "\n",
        "print(\"Finished!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeqknPsOO_wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sp_pystat = [[] for _ in range(len(results_pystat))]\n",
        "\n",
        "for i, sentence in enumerate(results_pystat):\n",
        "    for j, word in enumerate(sentence):\n",
        "      for k, tag in enumerate(word):\n",
        "        if tag == \"NP\" or tag == \"VP\":\n",
        "          sp_pystat[i].append(tag)\n",
        "          break\n",
        "      \n",
        "      try:\n",
        "        sp_pystat[i][j]\n",
        "      except:\n",
        "        sp_pystat[i].append(\"-PAD-\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXf9-Iq_ZAEm",
        "colab_type": "code",
        "outputId": "68722a62-860f-4551-b8da-4fd0ebc94d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(test_enc_pos))\n",
        "print(len(sp_pystat))\n",
        "print(len(test_sp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2604\n",
            "2604\n",
            "2604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYimJgJ3Srj6",
        "colab_type": "code",
        "outputId": "9c805603-f7ef-4833-903e-836e9d8e9004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "for i in reversed(range(len(test_sp))):\n",
        "  if len(sp_pystat[i]) != len(test_sp[i]):\n",
        "    print(i)\n",
        "    sp_pystat.pop(i)\n",
        "    test_enc_pos.pop(i)\n",
        "    test_sp.pop(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2547\n",
            "2146\n",
            "2133\n",
            "2120\n",
            "2096\n",
            "1920\n",
            "1376\n",
            "1122\n",
            "1070\n",
            "1067\n",
            "998\n",
            "742\n",
            "551\n",
            "469\n",
            "300\n",
            "284\n",
            "152\n",
            "81\n",
            "37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3POoWo1nYZtQ",
        "colab_type": "code",
        "outputId": "cd70f875-f2e1-4409-fce7-664ccf944f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(sp_pystat))\n",
        "print(len(test_enc_pos))\n",
        "print(len(test_sp))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2585\n",
            "2585\n",
            "2585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i6p4YiJYAVw",
        "colab_type": "code",
        "outputId": "1de43153-c691-49e1-ea37-963dc351879e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Convert data to numpy array\n",
        "test_enc_pos = np.array(test_enc_pos)\n",
        "    \n",
        "for i, v in enumerate(test_enc_pos):\n",
        "    v = np.array(v)\n",
        "    test_enc_pos[i] = v\n",
        "\n",
        "print(test_enc_pos.shape)\n",
        "\n",
        "for i in range(len(test_enc_pos)):\n",
        "  test_enc_pos[i] = test_enc_pos[i].reshape(1, len(test_enc_pos[i]), 1)\n",
        "\n",
        "for i in range(len(test_enc_pos)):\n",
        "  test_enc_pos[i] = np.array(test_enc_pos[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2585,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j48CXlNEkD8z",
        "colab_type": "code",
        "outputId": "f61dd98e-7885-4083-9d1a-ffe38aca38de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# TEST RESULTS COMPARISON\n",
        "import nltk\n",
        "\n",
        "# Variables to store accuracy of each sentence for both NLTK and Network\n",
        "pystat_accs = []\n",
        "network_accs = []\n",
        "\n",
        "# Function to decode network prediction\n",
        "def logits_to_tokens(sequences, index):\n",
        "  token_sequences = []\n",
        "  for categorical_sequence in sequences:\n",
        "    token_sequence = []\n",
        "    for categorical in categorical_sequence:\n",
        "      token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "      token_sequences.append(token_sequence)\n",
        "    \n",
        "  return token_sequences\n",
        "\n",
        "for i in range(len(test_enc_pos)):\n",
        "\n",
        "  # Network prediction requires encoded data\n",
        "  prediction = model2.predict(test_enc_pos[i], 1)\n",
        "\n",
        "  # Decode the network prediction\n",
        "  network_results = logits_to_tokens(prediction, {i: t for t, i in sp_index.items()})[0]\n",
        "\n",
        "  count_pystat = 0\n",
        "  count_network = 0\n",
        "\n",
        "  # Check number of correctly identified tags from Network results\n",
        "  for j in range(len(test_enc_pos[i][0])):\n",
        "    if test_sp[i][j] == sp_pystat[i][j]:\n",
        "      count_pystat += 1\n",
        "\n",
        "    if test_sp[i][j].lower() == network_results[j]:\n",
        "      count_network += 1\n",
        "\n",
        "  # Calculate accuracy for each sentence, and append to accuracy logs\n",
        "  acc_log_pystat = count_pystat / len(test_sp[i])\n",
        "  pystat_accs.append(acc_log_pystat)\n",
        "\n",
        "  acc_log_network = count_network / len(test_sp[i])\n",
        "  network_accs.append(acc_log_network)\n",
        "\n",
        "# Calculate final accuracy (average)\n",
        "network_acc = sum(network_accs) / len(test_sp)\n",
        "pystat_acc = sum(pystat_accs) / len(test_sp)\n",
        "\n",
        "print(\"Network Accuracy: \", network_acc*100, \"%\")\n",
        "print(\"PyStatParser Accuracy: \", pystat_acc*100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network Accuracy:  91.7432601469495 %\n",
            "PyStatParser Accuracy:  61.588736986968854 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ckhAKu3VTB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}